{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BTP - MemNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPq/Hy6j5CIKOItfpMqDVu8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZO9j3VF4Dao","executionInfo":{"status":"ok","timestamp":1619939765186,"user_tz":-330,"elapsed":1035,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"a86c78b5-593c-4280-ebce-f60a4289c7fd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7WlHe3hc4f5I","executionInfo":{"status":"ok","timestamp":1619939768581,"user_tz":-330,"elapsed":1365,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import torch.utils.data as data\n","import torch\n","import h5py\n","\n","class DatasetFromHdf5(data.Dataset):\n","    def __init__(self, file_path):\n","        super(DatasetFromHdf5, self).__init__()\n","        hf = h5py.File(file_path)\n","        self.data = hf.get('data')\n","        self.target = hf.get('label')\n","\n","    def __getitem__(self, index):\n","        return torch.from_numpy(self.data[index,:,:,:]).float(), torch.from_numpy(self.target[index,:,:,:]).float()\n","        \n","    def __len__(self):\n","        return self.data.shape[0]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_B-O-yFd4pCd","executionInfo":{"status":"ok","timestamp":1619939769482,"user_tz":-330,"elapsed":2093,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","#dtype = torch.FloatTensor\n","dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n","\n","class MemNet(nn.Module):\n","    def __init__(self, in_channels, channels, num_memblock, num_resblock):\n","        super(MemNet, self).__init__()\n","        self.feature_extractor = BNReLUConv(in_channels, channels, True)  #FENet: staic(bn)+relu+conv1\n","        self.reconstructor = BNReLUConv(channels, in_channels, True)      #ReconNet: static(bn)+relu+conv \n","        self.dense_memory = nn.ModuleList(\n","            [MemoryBlock(channels, num_resblock, i+1) for i in range(num_memblock)]\n","        )\n","        #ModuleList can be indexed like a regular Python list, but modules it contains are \n","        #properly registered, and will be visible by all Module methods.\n","        \n","        \n","        self.weights = nn.Parameter((torch.ones(1, num_memblock)/num_memblock), requires_grad=True)  \n","        #output1,...,outputn corresponding w1,...,w2\n","\n","\n","    #Multi-supervised MemNet architecture\n","    def forward(self, x):\n","        residual = x\n","        out = self.feature_extractor(x)\n","        w_sum=self.weights.sum(1)  \n","        mid_feat=[]   # A lsit contains the output of each memblock\n","        ys = [out]  #A list contains previous memblock output(long-term memory)  and the output of FENet\n","        for memory_block in self.dense_memory:\n","            out = memory_block(out, ys)  #out is the output of GateUnit  channels=64\n","            mid_feat.append(out);\n","        #pred = Variable(torch.zeros(x.shape).type(dtype),requires_grad=False)\n","        pred = (self.reconstructor(mid_feat[0])+residual)*self.weights.data[0][0]/w_sum\n","        for i in range(1,len(mid_feat)):\n","            pred = pred + (self.reconstructor(mid_feat[i])+residual)*self.weights.data[0][i]/w_sum\n","\n","        return pred\n","\n","    #Base MemNet architecture\n","    '''\n","    def forward(self, x):\n","        residual = x   #input data 1 channel\n","        out = self.feature_extractor(x)\n","        ys = [out]  #A list contains previous memblock output and the output of FENet\n","        for memory_block in self.dense_memory:\n","            out = memory_block(out, ys)\n","        out = self.reconstructor(out)\n","        out = out + residual\n","        \n","        return out\n","    '''\n","\n","\n","class MemoryBlock(nn.Module):\n","    \"\"\"Note: num_memblock denotes the number of MemoryBlock currently\"\"\"\n","    def __init__(self, channels, num_resblock, num_memblock):\n","        super(MemoryBlock, self).__init__()\n","        self.recursive_unit = nn.ModuleList(\n","            [ResidualBlock(channels) for i in range(num_resblock)]\n","        )\n","        #self.gate_unit = BNReLUConv((num_resblock+num_memblock) * channels, channels, True)  #kernel 3x3\n","        self.gate_unit = GateUnit((num_resblock+num_memblock) * channels, channels, True)   #kernel 1x1\n","\n","    def forward(self, x, ys):\n","        \"\"\"ys is a list which contains long-term memory coming from previous memory block\n","        xs denotes the short-term memory coming from recursive unit\n","        \"\"\"\n","        xs = []\n","        residual = x\n","        for layer in self.recursive_unit:\n","            x = layer(x)\n","            xs.append(x)\n","       \n","        \n","        #gate_out = self.gate_unit(torch.cat([xs,ys], dim=1))\n","        gate_out = self.gate_unit(torch.cat(xs+ys, 1))  #where xs and ys are list, so concat operation is xs+ys\n","        ys.append(gate_out)\n","        return gate_out\n","\n","\n","class ResidualBlock(torch.nn.Module):\n","    \"\"\"ResidualBlock\n","    introduced in: https://arxiv.org/abs/1512.03385\n","    x - Relu - Conv - Relu - Conv - x\n","    \"\"\"\n","\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        self.relu_conv1 = BNReLUConv(channels, channels, True)\n","        self.relu_conv2 = BNReLUConv(channels, channels, True)\n","        \n","    def forward(self, x):\n","        residual = x\n","        out = self.relu_conv1(x)\n","        out = self.relu_conv2(out)\n","        out = out + residual\n","        return out\n","\n","\n","class BNReLUConv(nn.Sequential):\n","    def __init__(self, in_channels, channels, inplace=True):\n","        super(BNReLUConv, self).__init__()\n","        self.add_module('bn', nn.BatchNorm2d(in_channels))\n","        self.add_module('relu', nn.ReLU(inplace=inplace))  #tureL: direct modified x, false: new object and the modified\n","        self.add_module('conv', nn.Conv2d(in_channels, channels, 3, 1, 1))  #bias: defautl: ture on pytorch, learnable bias\n","\n","class GateUnit(nn.Sequential):\n","    def __init__(self, in_channels, channels, inplace=True):\n","        super(GateUnit, self).__init__()\n","        self.add_module('bn',nn.BatchNorm2d(in_channels))\n","        self.add_module('relu', nn.ReLU(inplace=inplace))\n","        self.add_module('conv', nn.Conv2d(in_channels, channels,1,1,0))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSwS5khR4rjx","executionInfo":{"status":"ok","timestamp":1619939769485,"user_tz":-330,"elapsed":1968,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import os\n","import torch\n","import numpy as np\n","\n","from collections import OrderedDict\n","\n","\n","def convert_state_dict(state_dict):\n","    \"\"\"Converts a state dict saved from a dataParallel module to normal \n","       module state_dict inplace\n","       :param state_dict is the loaded DataParallel model_state\n","       You probably saved the model using nn.DataParallel, which stores the model in module, and now you are trying to load it \n","       without DataParallel. You can either add a nn.DataParallel temporarily in your network for loading purposes, or you can \n","       load the weights file, create a new ordered dict without the module prefix, and load it back \n","    \"\"\"\n","    state_dict_new = OrderedDict()\n","    #print(type(state_dict))\n","    for k, v in state_dict.items():\n","        #print(k)\n","        name = k[7:] # remove the prefix module.\n","        # My heart is borken, the pytorch have no ability to do with the problem.\n","        state_dict_new[name] = v\n","    return state_dict_new\n","\n","\n","\n","def calc_patch_size(func):\n","    def wrapper(args):\n","        if args.scale == 2:\n","            args.patch_size = 10\n","        elif args.scale == 3:\n","            args.patch_size = 7\n","        elif args.scale == 4:\n","            args.patch_size = 6\n","        else:\n","            raise Exception('Scale Error', args.scale)\n","        return func(args)\n","    return wrapper\n","\n","\n","def convert_rgb_to_y(img, dim_order='hwc'):\n","    if dim_order == 'hwc':\n","        return 16. + (64.738 * img[..., 0] + 129.057 * img[..., 1] + 25.064 * img[..., 2]) / 256.\n","    else:\n","        return 16. + (64.738 * img[0] + 129.057 * img[1] + 25.064 * img[2]) / 256.\n","\n","\n","def convert_rgb_to_ycbcr(img, dim_order='hwc'):\n","    if dim_order == 'hwc':\n","        y = 16. + (64.738 * img[..., 0] + 129.057 * img[..., 1] + 25.064 * img[..., 2]) / 256.\n","        cb = 128. + (-37.945 * img[..., 0] - 74.494 * img[..., 1] + 112.439 * img[..., 2]) / 256.\n","        cr = 128. + (112.439 * img[..., 0] - 94.154 * img[..., 1] - 18.285 * img[..., 2]) / 256.\n","    else:\n","        y = 16. + (64.738 * img[0] + 129.057 * img[1] + 25.064 * img[2]) / 256.\n","        cb = 128. + (-37.945 * img[0] - 74.494 * img[1] + 112.439 * img[2]) / 256.\n","        cr = 128. + (112.439 * img[0] - 94.154 * img[1] - 18.285 * img[2]) / 256.\n","    return np.array([y, cb, cr]).transpose([1, 2, 0])\n","\n","\n","def convert_ycbcr_to_rgb(img, dim_order='hwc'):\n","    if dim_order == 'hwc':\n","        r = 298.082 * img[..., 0] / 256. + 408.583 * img[..., 2] / 256. - 222.921\n","        g = 298.082 * img[..., 0] / 256. - 100.291 * img[..., 1] / 256. - 208.120 * img[..., 2] / 256. + 135.576\n","        b = 298.082 * img[..., 0] / 256. + 516.412 * img[..., 1] / 256. - 276.836\n","    else:\n","        r = 298.082 * img[0] / 256. + 408.583 * img[2] / 256. - 222.921\n","        g = 298.082 * img[0] / 256. - 100.291 * img[1] / 256. - 208.120 * img[2] / 256. + 135.576\n","        b = 298.082 * img[0] / 256. + 516.412 * img[1] / 256. - 276.836\n","    return np.array([r, g, b]).transpose([1, 2, 0])\n","\n","\n","def preprocess(img, device):\n","    img = np.array(img).astype(np.float32)\n","    ycbcr = convert_rgb_to_ycbcr(img)\n","    x = ycbcr[..., 0]\n","    x /= 255.\n","    x = torch.from_numpy(x).to(device)\n","    x = x.unsqueeze(0).unsqueeze(0)\n","    return x, ycbcr\n","\n","\n","def calc_psnr(img1, img2):\n","    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))\n","\n","\n","from scipy.ndimage import gaussian_filter\n","\n","def calc_ssim(img1, img2, sd=1.5, C1=0.01**2, C2=0.03**2):\n","    img1 = img1.cpu()\n","    img2 = img2.cpu()\n","    mu1 = gaussian_filter(img1, sd)\n","    mu2 = gaussian_filter(img2, sd)\n","    mu1_sq = mu1 * mu1\n","    mu2_sq = mu2 * mu2\n","    mu1_mu2 = mu1 * mu2\n","    sigma1_sq = gaussian_filter(img1 * img1, sd) - mu1_sq\n","    sigma2_sq = gaussian_filter(img2 * img2, sd) - mu2_sq\n","    sigma12 = gaussian_filter(img1 * img2, sd) - mu1_mu2\n","    \n","    ssim_num = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2))\n","    ssim_den = ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n","    ssim_map = ssim_num / ssim_den\n","    mssim = np.mean(ssim_map)\n","    \n","    return mssim\n","\n","\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyWg9lqU4trD","executionInfo":{"status":"ok","timestamp":1619939769487,"user_tz":-330,"elapsed":1737,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import os\n","import torch\n","import random\n","import torch.backends.cudnn as cudnn\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","\n","def adjust_learning_rate(optimizer, epoch):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every num(step) epochs\"\"\"\n","    lr = lr * (0.1 ** (epoch // step))\n","    return lr\n","\n","def train(training_data_loader, optimizer, model, criterion, epoch):\n","    lr = adjust_learning_rate(optimizer, epoch-1)\n","\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","    print(\"Epoch = {}, lr = {}\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n","\n","    model.train()\n","\n","    for iteration, batch in enumerate(training_data_loader, 1):\n","        input, target = Variable(batch[0]), Variable(batch[1], requires_grad=False)\n","\n","        if cuda:\n","            input = input.cuda()\n","            target = target.cuda()\n","\n","        #loss = criterion(model(input), target)\n","        prediction = model(input)\n","        loss = criterion(prediction, target)\n","        #print(\"Outside: input size\", input.size(),\"prediction_size\", prediction.size())\n","        optimizer.zero_grad()\n","        loss.backward() \n","        nn.utils.clip_grad_norm(model.parameters(),clip) \n","        optimizer.step()\n","\n","        if iteration%100 == 0:\n","            print(\"===> Epoch[{}]({}/{}): Loss: {:.10f}\".format(epoch, iteration, len(training_data_loader), loss.data[0]))\n","\n","def save_checkpoint(model, epoch):\n","    model_out_path = \"checkpoint1/\" + \"model_epoch_{}.pth\".format(epoch)\n","    #state = {\"epoch\": epoch ,\"model\": model.state_dict}\n","    state = {\"epoch\": epoch ,\"model\": model.state_dict()}\n","    if not os.path.exists(\"checkpoint1/\"):\n","        os.makedirs(\"checkpoint1/\")\n","\n","    torch.save(state, model_out_path)  # save weights and network architecture\n","\n","    print(\"Checkpoint saved to {}\".format(model_out_path))\n","\n","def train_main(batchSize=512, nEpochs=50, lr=0.1, step=20, resume='', seed=123, start_epoch=1, clip=0.4, threads=1, momentum=0.9, weight_decay=1e-4, pretrained='', gpus='0,1,2,3'):\n","    torch.manual_seed(seed)\n","    if cuda:\n","        torch.cuda.manual_seed(seed)\n","\n","    cudnn.benchmark = True\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","    print(\"===> Loading datasets\")\n","    train_set = DatasetFromHdf5(\"data/SuperResolution/train_291_31_x234.h5\")\n","    training_data_loader = DataLoader(dataset=train_set, num_workers=threads, batch_size=batchSize, shuffle=True)\n","\n","    print(\"===> Building model\")\n","    model = MemNet(1, 64, 6, 6)\n","    criterion = nn.MSELoss(size_average=False)\n","\n","    print(\"===> Setting GPU\")\n","    if cuda:\n","        #model = model.cuda()\n","        model = torch.nn.DataParallel(model).cuda()  #multi-card data parallel\n","        criterion = criterion.cuda()\n","\n","    # optionally resume from a checkpoint\n","    if resume:\n","        if os.path.isfile(resume):\n","            print(\"=> loading checkpoint '{}'\".format(resume))\n","            checkpoint = torch.load(resume)\n","            start_epoch = checkpoint[\"epoch\"] + 1\n","            model.load_state_dict(checkpoint[\"model\"])\n","        else:\n","            print(\"=> no checkpoint found at '{}'\".format(resume))\n","\n","    # optionally copy weights from a checkpoint\n","    if pretrained:\n","        if os.path.isfile(pretrained):\n","            print(\"=> loading model '{}'\".format(pretrained))\n","            weights = torch.load(pretrained)\n","            model.load_state_dict(weights['model'].state_dict())\n","            model.load_state_dict(weights['model'].state_dict())\n","        else:\n","            print(\"=> no model found at '{}'\".format(pretrained))  \n","\n","    print(\"===> Setting Optimizer\")\n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    print(\"===> Training\")\n","    for epoch in range(start_epoch, nEpochs + 1):\n","        train(training_data_loader, optimizer, model, criterion, epoch)\n","        save_checkpoint(model, epoch)\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIB5vl2m5J6v","executionInfo":{"status":"ok","timestamp":1619939769489,"user_tz":-330,"elapsed":1455,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import torch\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import PIL.Image as pil_image\n","\n","\n","import torch\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import PIL.Image as pil_image\n","\n","from torch.autograd import Variable\n","\n","def test(weights_file, image_file, scale, save=False, debug=False, B=1, U=9, num_features=128):\n","    cudnn.benchmark = True\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","    model = MemNet(in_channels=1, channels=64, num_memblock=6, num_resblock=6)\n","    model.load_state_dict(convert_state_dict(torch.load(weights_file)[\"model\"]))\n","\n","    model.eval()\n","    model.to(device)\n","\n","    image = pil_image.open(image_file).convert('RGB')\n","    image_file = os.path.basename(image_file)\n","\n","    image_width = (image.width // scale) * scale\n","    image_height = (image.height // scale) * scale\n","\n","    hr = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n","    lr = hr.resize((hr.width // scale, hr.height // scale), resample=pil_image.BICUBIC)\n","    bicubic = lr.resize((lr.width * scale, lr.height * scale), resample=pil_image.BICUBIC)\n","\n","    lr, _ = preprocess(lr, device)\n","    hr, _ = preprocess(hr, device)\n","    bicubic, ycbcr = preprocess(bicubic, device)\n","\n","    with torch.no_grad():\n","        # Pre-upsampling\n","        preds = model(bicubic).clamp(0.0, 1.0)\n","\n","    psnr = calc_psnr(hr, preds)\n","    ssim = calc_ssim(hr, preds)\n","\n","    if debug:\n","        print(f'PSNR/SSIM: {psnr:.2f}/{ssim:.4f}')\n","\n","    preds = preds.mul(255.0).cpu().numpy().squeeze(0).squeeze(0)\n","\n","    output = np.array([preds, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n","    output = np.clip(convert_ycbcr_to_rgb(output), 0.0, 255.0).astype(np.uint8)\n","    output = pil_image.fromarray(output)\n","    if save:\n","        save_path = f'/content/drive/Shareddrives/BTP Meets/results/Set5/{scale}x/{image_file}'\n","        output.save(save_path.replace('.', '_memnet.'))\n","    return float(psnr), float(ssim)\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXcgjlwgs69h","executionInfo":{"status":"ok","timestamp":1619939769491,"user_tz":-330,"elapsed":1278,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import os\n","\n","def do_test(psnr, ssim, BASE_DIR, save=False, debug=False):\n","    scales = [2, 3, 4]\n","\n","    for file in os.listdir(BASE_DIR):\n","        if file.endswith(\".png\"):\n","            image_file_path = os.path.join(BASE_DIR, file)\n","            if debug:\n","                print(file)\n","            for scale in scales:\n","                if debug:\n","                    print(f\"Scale: {scale}\")\n","                result = test(f'/content/drive/Shareddrives/BTP Meets/models/memnet.pth', image_file_path, scale, save, debug)\n","                if scale not in psnr:\n","                    psnr[scale] = []\n","                if scale not in ssim:\n","                    ssim[scale] = []\n","                psnr[scale].append(result[0])\n","                ssim[scale].append(result[1])\n","            if debug:\n","                print()\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaKQNBso5y1a","executionInfo":{"status":"ok","timestamp":1619939783004,"user_tz":-330,"elapsed":14672,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"428e9912-d466-4b34-fca8-7fc10ef1a90e"},"source":["psnr = {}\n","ssim = {}\n","do_test(psnr, ssim, '/content/drive/Shareddrives/BTP Meets/datasets/test/Set5/', True, True)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["head.png\n","Scale: 2\n","PSNR/SSIM: 35.52/0.8857\n","Scale: 3\n","PSNR/SSIM: 34.87/0.8717\n","Scale: 4\n","PSNR/SSIM: 32.49/0.7855\n","\n","butterfly.png\n","Scale: 2\n","PSNR/SSIM: 33.28/0.9693\n","Scale: 3\n","PSNR/SSIM: 29.20/0.9356\n","Scale: 4\n","PSNR/SSIM: 26.52/0.8870\n","\n","bird.png\n","Scale: 2\n","PSNR/SSIM: 41.01/0.9847\n","Scale: 3\n","PSNR/SSIM: 36.06/0.9598\n","Scale: 4\n","PSNR/SSIM: 32.17/0.9148\n","\n","baby.png\n","Scale: 2\n","PSNR/SSIM: 38.23/0.9624\n","Scale: 3\n","PSNR/SSIM: 35.75/0.9358\n","Scale: 4\n","PSNR/SSIM: 32.99/0.8859\n","\n","woman.png\n","Scale: 2\n","PSNR/SSIM: 35.57/0.9687\n","Scale: 3\n","PSNR/SSIM: 32.33/0.9402\n","Scale: 4\n","PSNR/SSIM: 29.48/0.8980\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S02QmYlA7Q1s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619939783007,"user_tz":-330,"elapsed":14518,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"bff88194-5493-4f7e-8b71-f73383f2d1cf"},"source":["import statistics\n","\n","scales = [2, 3, 4]\n","for scale in scales:\n","    print(f'Avg PSNR/SSIM {scale}x: {statistics.mean(psnr[scale]):.2f}/{statistics.mean(ssim[scale]):.4f}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Avg PSNR/SSIM 2x: 36.72/0.9542\n","Avg PSNR/SSIM 3x: 33.64/0.9286\n","Avg PSNR/SSIM 4x: 30.73/0.8742\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"pN_gP_UluGnv","executionInfo":{"status":"ok","timestamp":1619940648801,"user_tz":-330,"elapsed":879952,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"e6c2ca6c-c264-4a06-a18c-a0ce7b216057"},"source":["scales = [2, 3, 4]\n","\n","def calc_result(dataset):\n","    print()\n","    print(dataset)\n","    psnr = {}\n","    ssim = {}\n","    do_test(psnr, ssim, f'/content/drive/Shareddrives/BTP Meets/datasets/test/{dataset}/')\n","    for scale in scales:\n","        print(f'Avg PSNR/SSIM {scale}x: {statistics.mean(psnr[scale]):.2f}/{statistics.mean(ssim[scale]):.4f}')\n","\n","# calc_result('Set14')\n","# calc_result('BSDS100')\n","# calc_result('Manga109')\n","calc_result('Urban100')\n","\n","'''\n","Set14\n","Avg PSNR/SSIM 2x: 32.54/0.9110\n","Avg PSNR/SSIM 3x: 29.66/0.8468\n","Avg PSNR/SSIM 4x: 27.41/0.7648\n","\n","BSDS100\n","Avg PSNR/SSIM 2x: 33.10/0.9184\n","Avg PSNR/SSIM 3x: 29.02/0.8139\n","Avg PSNR/SSIM 4x: 27.79/0.7489\n","\n","Manga109\n","Avg PSNR/SSIM 2x: 36.16/0.9699\n","Avg PSNR/SSIM 3x: 31.08/0.9312\n","Avg PSNR/SSIM 4x: 28.28/0.8857\n","\n","Urban100\n","Avg PSNR/SSIM 2x: 30.69/0.9153\n","Avg PSNR/SSIM 3x: 27.66/0.8508\n","Avg PSNR/SSIM 4x: 24.74/0.7500\n","'''"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n","Urban100\n","Avg PSNR/SSIM 2x: 30.69/0.9153\n","Avg PSNR/SSIM 3x: 27.66/0.8508\n","Avg PSNR/SSIM 4x: 24.74/0.7500\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nSet14\\nAvg PSNR/SSIM 2x: 32.54/0.9110\\nAvg PSNR/SSIM 3x: 29.66/0.8468\\nAvg PSNR/SSIM 4x: 27.41/0.7648\\n\\nBSDS100\\nAvg PSNR/SSIM 2x: 33.10/0.9184\\nAvg PSNR/SSIM 3x: 29.02/0.8139\\nAvg PSNR/SSIM 4x: 27.79/0.7489\\n\\nManga109\\nAvg PSNR/SSIM 2x: 36.16/0.9699\\nAvg PSNR/SSIM 3x: 31.08/0.9312\\nAvg PSNR/SSIM 4x: 28.28/0.8857\\n'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"4E-AzKjouH2g"},"source":[""],"execution_count":null,"outputs":[]}]}