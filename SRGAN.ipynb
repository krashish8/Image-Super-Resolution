{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BTP - SRGAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOszRF+8MvmdHBsk/oI2NyE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4bab9ac77dad4178a087102c7688e0db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3a3474fa729d4fe78f669f966d5be1f2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7213f802477b41f1b38e3fd75e29fd5f","IPY_MODEL_27df58b8b8fa4602b83403399bf8f7db"]}},"3a3474fa729d4fe78f669f966d5be1f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7213f802477b41f1b38e3fd75e29fd5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1cc1b2b96a3a4819815e841a1121f075","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553433881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e5223e9286247a0964cbf31b55b008b"}},"27df58b8b8fa4602b83403399bf8f7db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ec14100fcef3411aa1ae1cf5d0731fd9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [00:16&lt;00:00, 34.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bc3ec4c3912b4531a6253c5c542b1db2"}},"1cc1b2b96a3a4819815e841a1121f075":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5e5223e9286247a0964cbf31b55b008b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec14100fcef3411aa1ae1cf5d0731fd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bc3ec4c3912b4531a6253c5c542b1db2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZO9j3VF4Dao","executionInfo":{"status":"ok","timestamp":1620037062356,"user_tz":-330,"elapsed":24461,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"1f696d27-18d5-4a4c-d684-1afbbdc0d9d6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WgFGXaP8sg9C","executionInfo":{"status":"ok","timestamp":1620037066200,"user_tz":-330,"elapsed":13416,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import math\n","\n","import torch.nn.functional as F\n","from torch import nn\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, scale_factor):\n","        upsample_block_num = int(math.log(scale_factor, 2))\n","\n","        super(Generator, self).__init__()\n","        self.block1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n","            nn.PReLU()\n","        )\n","        self.block2 = ResidualBlock(64)\n","        self.block3 = ResidualBlock(64)\n","        self.block4 = ResidualBlock(64)\n","        self.block5 = ResidualBlock(64)\n","        self.block6 = ResidualBlock(64)\n","        self.block7 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.PReLU()\n","        )\n","        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n","        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n","        self.block8 = nn.Sequential(*block8)\n","\n","    def forward(self, x):\n","        block1 = self.block1(x)\n","        block2 = self.block2(block1)\n","        block3 = self.block3(block2)\n","        block4 = self.block4(block3)\n","        block5 = self.block5(block4)\n","        block6 = self.block6(block5)\n","        block7 = self.block7(block6)\n","        block8 = self.block8(block1 + block7)\n","\n","        return (F.tanh(block8) + 1) / 2\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(512, 1024, kernel_size=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(1024, 1, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        return F.sigmoid(self.net(x).view(batch_size))\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.prelu = nn.PReLU()\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        residual = self.conv1(x)\n","        residual = self.bn1(residual)\n","        residual = self.prelu(residual)\n","        residual = self.conv2(residual)\n","        residual = self.bn2(residual)\n","\n","        return x + residual\n","\n","\n","class UpsampleBLock(nn.Module):\n","    def __init__(self, in_channels, up_scale):\n","        super(UpsampleBLock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n","        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n","        self.prelu = nn.PReLU()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.pixel_shuffle(x)\n","        x = self.prelu(x)\n","        return x"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPoCZa_0tlJI","executionInfo":{"status":"ok","timestamp":1620037066201,"user_tz":-330,"elapsed":12961,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import torch\n","import numpy as np\n","from scipy.ndimage import gaussian_filter\n","\n","\n","def calc_patch_size(func):\n","    def wrapper(args):\n","        if scale == 2:\n","            patch_size = 10\n","        elif scale == 3:\n","            patch_size = 7\n","        elif scale == 4:\n","            patch_size = 6\n","        else:\n","            raise Exception('Scale Error', scale)\n","        return func(args)\n","    return wrapper\n","\n","\n","def convert_rgb_to_y(img, dim_order='hwc'):\n","    if dim_order == 'hwc':\n","        return 16. + (64.738 * img[..., 0] + 129.057 * img[..., 1] + 25.064 * img[..., 2]) / 256.\n","    else:\n","        return 16. + (64.738 * img[0] + 129.057 * img[1] + 25.064 * img[2]) / 256.\n","\n","\n","def convert_rgb_to_ycbcr(img, dim_order='hwc'):\n","    if dim_order == 'hwc':\n","        y = 16. + (64.738 * img[..., 0] + 129.057 * img[..., 1] + 25.064 * img[..., 2]) / 256.\n","        cb = 128. + (-37.945 * img[..., 0] - 74.494 * img[..., 1] + 112.439 * img[..., 2]) / 256.\n","        cr = 128. + (112.439 * img[..., 0] - 94.154 * img[..., 1] - 18.285 * img[..., 2]) / 256.\n","    else:\n","        y = 16. + (64.738 * img[0] + 129.057 * img[1] + 25.064 * img[2]) / 256.\n","        cb = 128. + (-37.945 * img[0] - 74.494 * img[1] + 112.439 * img[2]) / 256.\n","        cr = 128. + (112.439 * img[0] - 94.154 * img[1] - 18.285 * img[2]) / 256.\n","    return np.array([y, cb, cr]).transpose([1, 2, 0])\n","\n","\n","def convert_ycbcr_to_rgb(img, dim_order='hwc'):\n","    if dim_order == 'hwc':\n","        r = 298.082 * img[..., 0] / 256. + 408.583 * img[..., 2] / 256. - 222.921\n","        g = 298.082 * img[..., 0] / 256. - 100.291 * img[..., 1] / 256. - 208.120 * img[..., 2] / 256. + 135.576\n","        b = 298.082 * img[..., 0] / 256. + 516.412 * img[..., 1] / 256. - 276.836\n","    else:\n","        r = 298.082 * img[0] / 256. + 408.583 * img[2] / 256. - 222.921\n","        g = 298.082 * img[0] / 256. - 100.291 * img[1] / 256. - 208.120 * img[2] / 256. + 135.576\n","        b = 298.082 * img[0] / 256. + 516.412 * img[1] / 256. - 276.836\n","    return np.array([r, g, b]).transpose([1, 2, 0])\n","\n","\n","def preprocess(img, device):\n","    img = np.array(img).astype(np.float32)\n","    ycbcr = convert_rgb_to_ycbcr(img)\n","    x = ycbcr[..., 0]\n","    x /= 255.\n","    x = torch.from_numpy(x).to(device)\n","    x = x.unsqueeze(0).unsqueeze(0)\n","    return x, ycbcr\n","\n","\n","def calc_psnr(img1, img2):\n","    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))\n","\n","\n","def calc_ssim(img1, img2, sd=1.5, C1=0.01**2, C2=0.03**2):\n","    img1 = img1.cpu()\n","    img2 = img2.cpu()\n","    mu1 = gaussian_filter(img1, sd)\n","    mu2 = gaussian_filter(img2, sd)\n","    mu1_sq = mu1 * mu1\n","    mu2_sq = mu2 * mu2\n","    mu1_mu2 = mu1 * mu2\n","    sigma1_sq = gaussian_filter(img1 * img1, sd) - mu1_sq\n","    sigma2_sq = gaussian_filter(img2 * img2, sd) - mu2_sq\n","    sigma12 = gaussian_filter(img1 * img2, sd) - mu1_mu2\n","    \n","    ssim_num = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2))\n","    ssim_den = ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n","    ssim_map = ssim_num / ssim_den\n","    mssim = np.mean(ssim_map)\n","    \n","    return mssim\n","\n","\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":712,"referenced_widgets":["4bab9ac77dad4178a087102c7688e0db","3a3474fa729d4fe78f669f966d5be1f2","7213f802477b41f1b38e3fd75e29fd5f","27df58b8b8fa4602b83403399bf8f7db","1cc1b2b96a3a4819815e841a1121f075","5e5223e9286247a0964cbf31b55b008b","ec14100fcef3411aa1ae1cf5d0731fd9","bc3ec4c3912b4531a6253c5c542b1db2"]},"id":"_Yk86UAetx16","executionInfo":{"status":"ok","timestamp":1620037070456,"user_tz":-330,"elapsed":16842,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"30c10e86-f377-4cc8-9d1b-a59824a90cf7"},"source":["import torch\n","from torch import nn\n","from torchvision.models.vgg import vgg16\n","\n","\n","class GeneratorLoss(nn.Module):\n","    def __init__(self):\n","        super(GeneratorLoss, self).__init__()\n","        vgg = vgg16(pretrained=True)\n","        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n","        for param in loss_network.parameters():\n","            param.requires_grad = False\n","        self.loss_network = loss_network\n","        self.mse_loss = nn.MSELoss()\n","        self.tv_loss = TVLoss()\n","\n","    def forward(self, out_labels, out_images, target_images):\n","        # Adversarial Loss\n","        adversarial_loss = torch.mean(1 - out_labels)\n","        # Perception Loss\n","        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n","        # Image Loss\n","        image_loss = self.mse_loss(out_images, target_images)\n","        # TV Loss\n","        tv_loss = self.tv_loss(out_images)\n","        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n","\n","\n","class TVLoss(nn.Module):\n","    def __init__(self, tv_loss_weight=1):\n","        super(TVLoss, self).__init__()\n","        self.tv_loss_weight = tv_loss_weight\n","\n","    def forward(self, x):\n","        batch_size = x.size()[0]\n","        h_x = x.size()[2]\n","        w_x = x.size()[3]\n","        count_h = self.tensor_size(x[:, :, 1:, :])\n","        count_w = self.tensor_size(x[:, :, :, 1:])\n","        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n","        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n","        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n","\n","    @staticmethod\n","    def tensor_size(t):\n","        return t.size()[1] * t.size()[2] * t.size()[3]\n","\n","\n","g_loss = GeneratorLoss()\n","print(g_loss)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bab9ac77dad4178a087102c7688e0db","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","GeneratorLoss(\n","  (loss_network): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (mse_loss): MSELoss()\n","  (tv_loss): TVLoss()\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gJrGH6DKuD_P","executionInfo":{"status":"ok","timestamp":1620037070457,"user_tz":-330,"elapsed":16501,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["from os import listdir\n","from os.path import join\n","\n","from PIL import Image\n","from torch.utils.data.dataset import Dataset\n","from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n","\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n","\n","\n","def calculate_valid_crop_size(crop_size, upscale_factor):\n","    return crop_size - (crop_size % upscale_factor)\n","\n","\n","def train_hr_transform(crop_size):\n","    return Compose([\n","        RandomCrop(crop_size),\n","        ToTensor(),\n","    ])\n","\n","\n","def train_lr_transform(crop_size, upscale_factor):\n","    return Compose([\n","        ToPILImage(),\n","        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n","        ToTensor()\n","    ])\n","\n","\n","def display_transform():\n","    return Compose([\n","        ToPILImage(),\n","        Resize(400),\n","        CenterCrop(400),\n","        ToTensor()\n","    ])\n","\n","\n","class TrainDatasetFromFolder(Dataset):\n","    def __init__(self, dataset_dir, crop_size, upscale_factor):\n","        super(TrainDatasetFromFolder, self).__init__()\n","        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n","        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n","        self.hr_transform = train_hr_transform(crop_size)\n","        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n","\n","    def __getitem__(self, index):\n","        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n","        lr_image = self.lr_transform(hr_image)\n","        return lr_image, hr_image\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","\n","class ValDatasetFromFolder(Dataset):\n","    def __init__(self, dataset_dir, upscale_factor):\n","        super(ValDatasetFromFolder, self).__init__()\n","        self.upscale_factor = upscale_factor\n","        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n","\n","    def __getitem__(self, index):\n","        hr_image = Image.open(self.image_filenames[index])\n","        w, h = hr_image.size\n","        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n","        lr_scale = Resize(crop_size // self.upscale_factor, interpolation=Image.BICUBIC)\n","        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n","        hr_image = CenterCrop(crop_size)(hr_image)\n","        lr_image = lr_scale(hr_image)\n","        hr_restore_img = hr_scale(lr_image)\n","        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","\n","class TestDatasetFromFolder(Dataset):\n","    def __init__(self, dataset_dir, upscale_factor):\n","        super(TestDatasetFromFolder, self).__init__()\n","        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/'\n","        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/'\n","        self.upscale_factor = upscale_factor\n","        self.lr_filenames = [join(self.lr_path, x) for x in listdir(self.lr_path) if is_image_file(x)]\n","        self.hr_filenames = [join(self.hr_path, x) for x in listdir(self.hr_path) if is_image_file(x)]\n","\n","    def __getitem__(self, index):\n","        image_name = self.lr_filenames[index].split('/')[-1]\n","        lr_image = Image.open(self.lr_filenames[index])\n","        w, h = lr_image.size\n","        hr_image = Image.open(self.hr_filenames[index])\n","        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w), interpolation=Image.BICUBIC)\n","        hr_restore_img = hr_scale(lr_image)\n","        return image_name, ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n","\n","    def __len__(self):\n","        return len(self.lr_filenames)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nm-IBr7guJcn","executionInfo":{"status":"ok","timestamp":1620037070458,"user_tz":-330,"elapsed":15837,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import os\n","from math import log10\n","\n","import pandas as pd\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.utils as utils\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","def train(crop_size=88, upscale_factor=4, num_epochs=100):\n","    CROP_SIZE = crop_size\n","    UPSCALE_FACTOR = upscale_factor\n","    NUM_EPOCHS = num_epochs\n","\n","    train_set = TrainDatasetFromFolder('data/VOC2012/train', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n","    val_set = ValDatasetFromFolder('data/VOC2012/val', upscale_factor=UPSCALE_FACTOR)\n","    train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=64, shuffle=True)\n","    val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)\n","\n","    netG = Generator(UPSCALE_FACTOR)\n","    print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n","    netD = Discriminator()\n","    print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n","\n","    generator_criterion = GeneratorLoss()\n","\n","    if torch.cuda.is_available():\n","        netG.cuda()\n","        netD.cuda()\n","        generator_criterion.cuda()\n","\n","    optimizerG = optim.Adam(netG.parameters())\n","    optimizerD = optim.Adam(netD.parameters())\n","\n","    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n","\n","    for epoch in range(1, NUM_EPOCHS + 1):\n","        train_bar = tqdm(train_loader)\n","        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n","\n","        netG.train()\n","        netD.train()\n","        for data, target in train_bar:\n","            g_update_first = True\n","            batch_size = data.size(0)\n","            running_results['batch_sizes'] += batch_size\n","\n","            ############################\n","            # (1) Update D network: maximize D(x)-1-D(G(z))\n","            ###########################\n","            real_img = Variable(target)\n","            if torch.cuda.is_available():\n","                real_img = real_img.cuda()\n","            z = Variable(data)\n","            if torch.cuda.is_available():\n","                z = z.cuda()\n","            fake_img = netG(z)\n","\n","            netD.zero_grad()\n","            real_out = netD(real_img).mean()\n","            fake_out = netD(fake_img).mean()\n","            d_loss = 1 - real_out + fake_out\n","            d_loss.backward(retain_graph=True)\n","            optimizerD.step()\n","\n","            ############################\n","            # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n","            ###########################\n","            netG.zero_grad()\n","            g_loss = generator_criterion(fake_out, fake_img, real_img)\n","            g_loss.backward()\n","            optimizerG.step()\n","            fake_img = netG(z)\n","            fake_out = netD(fake_img).mean()\n","\n","            g_loss = generator_criterion(fake_out, fake_img, real_img)\n","            running_results['g_loss'] += g_loss.data[0] * batch_size\n","            d_loss = 1 - real_out + fake_out\n","            running_results['d_loss'] += d_loss.data[0] * batch_size\n","            running_results['d_score'] += real_out.data[0] * batch_size\n","            running_results['g_score'] += fake_out.data[0] * batch_size\n","\n","            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n","                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n","                running_results['g_loss'] / running_results['batch_sizes'],\n","                running_results['d_score'] / running_results['batch_sizes'],\n","                running_results['g_score'] / running_results['batch_sizes']))\n","\n","        netG.eval()\n","        out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n","        if not os.path.exists(out_path):\n","            os.makedirs(out_path)\n","        val_bar = tqdm(val_loader)\n","        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n","        val_images = []\n","        for val_lr, val_hr_restore, val_hr in val_bar:\n","            batch_size = val_lr.size(0)\n","            valing_results['batch_sizes'] += batch_size\n","            lr = Variable(val_lr, volatile=True)\n","            hr = Variable(val_hr, volatile=True)\n","            if torch.cuda.is_available():\n","                lr = lr.cuda()\n","                hr = hr.cuda()\n","            sr = netG(lr)\n","\n","            batch_mse = ((sr - hr) ** 2).data.mean()\n","            valing_results['mse'] += batch_mse * batch_size\n","            batch_ssim = pytorch_ssim.ssim(sr, hr).data[0]\n","            valing_results['ssims'] += batch_ssim * batch_size\n","            valing_results['psnr'] = 10 * log10(1 / (valing_results['mse'] / valing_results['batch_sizes']))\n","            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n","            val_bar.set_description(\n","                desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n","                    valing_results['psnr'], valing_results['ssim']))\n","\n","            val_images.extend(\n","                [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n","                display_transform()(sr.data.cpu().squeeze(0))])\n","        val_images = torch.stack(val_images)\n","        val_images = torch.chunk(val_images, val_images.size(0) // 15)\n","        val_save_bar = tqdm(val_images, desc='[saving training results]')\n","        index = 1\n","        for image in val_save_bar:\n","            image = utils.make_grid(image, nrow=3, padding=5)\n","            utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n","            index += 1\n","\n","        # save model parameters\n","        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n","        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n","        # save loss\\scores\\psnr\\ssim\n","        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n","        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n","        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n","        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n","        results['psnr'].append(valing_results['psnr'])\n","        results['ssim'].append(valing_results['ssim'])\n","\n","        if epoch % 10 == 0 and epoch != 0:\n","            out_path = 'statistics/'\n","            data_frame = pd.DataFrame(\n","                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n","                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n","                index=range(1, epoch + 1))\n","            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNrMW36RszZH","executionInfo":{"status":"ok","timestamp":1620037070459,"user_tz":-330,"elapsed":15170,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import torch\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import PIL.Image as pil_image\n","\n","from torch.autograd import Variable\n","\n","\n","def test(weights_file, image_file, scale, save=False, debug=False, B=1, U=9, num_features=128):\n","    cudnn.benchmark = True\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","    model = Generator(scale)\n","    model.load_state_dict(torch.load(weights_file, map_location=device))\n","\n","    model.eval()\n","    model.to(device)\n","\n","    image = pil_image.open(image_file).convert('RGB')\n","    image_file = os.path.basename(image_file)\n","\n","    image_width = (image.width // scale) * scale\n","    image_height = (image.height // scale) * scale\n","\n","    hr = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n","    lr = hr.resize((hr.width // scale, hr.height // scale), resample=pil_image.BICUBIC)\n","    bicubic = lr.resize((lr.width * scale, lr.height * scale), resample=pil_image.BICUBIC)\n","\n","    hr = np.array(hr).astype(float)\n","    bicubic = np.array(bicubic).astype(float)\n","    im_l = np.array(lr).astype(float)\n","\n","    hr, _ = preprocess(hr, device)\n","    _, ycbcr = preprocess(bicubic, device)\n","\n","    im_input = im_l.astype(np.float32).transpose(2,0,1)\n","    im_input = im_input.reshape(1,im_input.shape[0],im_input.shape[1],im_input.shape[2])\n","    im_input = Variable(torch.from_numpy(im_input/255.).float()).to(device)\n","\n","    with torch.no_grad():\n","        preds = model(im_input)\n","\n","    preds = preds.cpu().data[0].numpy().astype(np.float32)\n","    preds = preds*255.\n","    preds = np.clip(preds, 0., 255.)\n","    preds = preds.transpose(1,2,0).astype(np.float32)\n","\n","    preds, _ = preprocess(preds, device)\n","\n","    psnr = calc_psnr(hr, preds)\n","    ssim = calc_ssim(hr, preds)\n","\n","    if debug:\n","        print(f'PSNR/SSIM: {psnr:.2f}/{ssim:.4f}')\n","\n","    preds = preds.mul(255.0).cpu().numpy().squeeze(0).squeeze(0)\n","\n","    output = np.array([preds, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n","    output = np.clip(convert_ycbcr_to_rgb(output), 0.0, 255.0).astype(np.uint8)\n","    output = pil_image.fromarray(output)\n","    if save:\n","        save_path = f'/content/drive/Shareddrives/BTP Meets/results/Set5/{scale}x/{image_file}'\n","        output.save(save_path.replace('.', '_srgan.'))\n","    return float(psnr), float(ssim)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIuKKAC4s72G","executionInfo":{"status":"ok","timestamp":1620037070460,"user_tz":-330,"elapsed":14586,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}}},"source":["import os\n","\n","def do_test(psnr, ssim, BASE_DIR, save=False, debug=False):\n","    scales = [2, 4, 8]\n","\n","    for file in os.listdir(BASE_DIR):\n","        if file.endswith(\".png\"):\n","            image_file_path = os.path.join(BASE_DIR, file)\n","            if debug:\n","                print(file)\n","            for scale in scales:\n","                if debug:\n","                    print(f\"Scale: {scale}\")\n","                result = test(f'/content/drive/Shareddrives/BTP Meets/models/netG_epoch_{scale}_100.pth', image_file_path, scale, save, debug)\n","                if scale not in psnr:\n","                    psnr[scale] = []\n","                if scale not in ssim:\n","                    ssim[scale] = []\n","                psnr[scale].append(result[0])\n","                ssim[scale].append(result[1])\n","            if debug:\n","                print()\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEngq8-htDRq","executionInfo":{"status":"ok","timestamp":1620037096378,"user_tz":-330,"elapsed":37843,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"98fcaa4a-efb5-4c16-8f0e-e9e0387a541a"},"source":["psnr = {}\n","ssim = {}\n","do_test(psnr, ssim, '/content/drive/Shareddrives/BTP Meets/datasets/test/Set5/', True, True)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["head.png\n","Scale: 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["PSNR/SSIM: 35.05/0.8763\n","Scale: 4\n","PSNR/SSIM: 32.34/0.7806\n","Scale: 8\n","PSNR/SSIM: 29.97/0.6951\n","\n","butterfly.png\n","Scale: 2\n","PSNR/SSIM: 32.36/0.9548\n","Scale: 4\n","PSNR/SSIM: 26.26/0.8662\n","Scale: 8\n","PSNR/SSIM: 20.63/0.6780\n","\n","bird.png\n","Scale: 2\n","PSNR/SSIM: 38.08/0.9682\n","Scale: 4\n","PSNR/SSIM: 32.51/0.9055\n","Scale: 8\n","PSNR/SSIM: 26.90/0.7515\n","\n","baby.png\n","Scale: 2\n","PSNR/SSIM: 37.26/0.9531\n","Scale: 4\n","PSNR/SSIM: 32.60/0.8721\n","Scale: 8\n","PSNR/SSIM: 28.62/0.7691\n","\n","woman.png\n","Scale: 2\n","PSNR/SSIM: 34.82/0.9615\n","Scale: 4\n","PSNR/SSIM: 29.46/0.8868\n","Scale: 8\n","PSNR/SSIM: 24.58/0.7608\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxplMRJNtE0L","executionInfo":{"status":"ok","timestamp":1620037099239,"user_tz":-330,"elapsed":1277,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"ec4a7c0e-0674-4389-9d75-c02126c1d745"},"source":["import statistics\n","\n","scales = [2, 4, 8]\n","for scale in scales:\n","    print(f'Avg PSNR/SSIM {scale}x: {statistics.mean(psnr[scale]):.2f}/{statistics.mean(ssim[scale]):.4f}')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Avg PSNR/SSIM 2x: 35.51/0.9428\n","Avg PSNR/SSIM 4x: 30.63/0.8623\n","Avg PSNR/SSIM 8x: 26.14/0.7309\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hT9LzegXu_l8","executionInfo":{"status":"ok","timestamp":1620037787238,"user_tz":-330,"elapsed":637789,"user":{"displayName":"ASHISH KUMAR 4-Yr B.Tech. Comp. Sci. and Engg, 2018-2022","photoUrl":"","userId":"09157645467553063630"}},"outputId":"9b7d02da-8108-4146-e157-602aab2eaaa0"},"source":["scales = [2, 4, 8]\n","\n","def calc_result(dataset):\n","    print()\n","    print(dataset)\n","    psnr = {}\n","    ssim = {}\n","    do_test(psnr, ssim, f'/content/drive/Shareddrives/BTP Meets/datasets/test/{dataset}/')\n","    for scale in scales:\n","        print(f'Avg PSNR/SSIM {scale}x: {statistics.mean(psnr[scale]):.2f}/{statistics.mean(ssim[scale]):.4f}')\n","\n","calc_result('Set14')\n","calc_result('BSDS100')\n","calc_result('Manga109')\n","calc_result('Urban100')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["\n","Set14\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Avg PSNR/SSIM 2x: 32.13/0.9009\n","Avg PSNR/SSIM 4x: 27.57/0.7577\n","Avg PSNR/SSIM 8x: 24.33/0.6229\n","\n","BSDS100\n","Avg PSNR/SSIM 2x: 32.71/0.9102\n","Avg PSNR/SSIM 4x: 27.85/0.7403\n","Avg PSNR/SSIM 8x: 25.05/0.6014\n","\n","Manga109\n","Avg PSNR/SSIM 2x: 34.90/0.9620\n","Avg PSNR/SSIM 4x: 28.94/0.8784\n","Avg PSNR/SSIM 8x: 23.93/0.7473\n","\n","Urban100\n","Avg PSNR/SSIM 2x: 30.15/0.9055\n","Avg PSNR/SSIM 4x: 25.06/0.7435\n","Avg PSNR/SSIM 8x: 22.02/0.5844\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iH7xI8-tvCMi"},"source":[""],"execution_count":null,"outputs":[]}]}